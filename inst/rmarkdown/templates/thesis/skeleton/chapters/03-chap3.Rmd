# METHODS 

Two factors varied in the simulation study were sample size and measurement model. The two measurement models included were the binary Rasch model (Equation 2-1) and the 2PL model (Equation 2-2). For the Rasch model conditions, sample size was varied as n = 50, 100, and 250 to mimic measurement applications ranging from minimum sample size required for data calibration to a relatively typical sample for data calibration under that model. For the 2PL conditions, n = 100, 250, and 500 were included for similar purposes.  

## Rasch Model 
### Data Generation 
Item responses were simulated for a thirty item dichotomously scored test using Rasch models in R version 3.2.3 (R Core Team, 2015) and following Linacre’s (2007) guidelines for simulating Rasch data. Thirty items were created for each iteration with an item difficulty ranging from -2.9 to 2.9 logits, separated by 0.2 logits. No two items within an iteration had the same difficulty and each item was assumed to measure the same construct (i.e., the test data was unidimensional). Three sample sizes of examinees were simulated (i.e., n = 50, 100, and 250) from $\theta \sim N(0,1)$, with the smallest sample size chosen according to Linacre (1994) and the larger sample sizes chosen to mimic the typical sample sizes used in studies piloting Rasch-modelled instruments (e.g., Chen et al., 2014). Then, a 0/1 outcome variable $y$ was developed as an external criterion that is, in this study, perfectly correlated with true $\theta$ such that (equation 3-1). Then the response vector on all 30 items for all n was generated according to the following steps: 

1.	Generate a random number $U_{ij} \sim Unif(0,1)$ for person i = 1 on item j = 1 with 
difficulty $d_j$. 
2.	Generate the probability of failure 0 by $P(0)=\frac{1}{1+\exp{\theta_i-d_j}}$. 
3.	If $U_{i1} > P(0)$, then response outcome $x_{ij}=1$, else $x_{ij} = 0$.
4.	Repeat steps 1 through 3 for person i on the remaining items, creating a vector of responses. 
5.	Repeat for remaining persons, creating the response matrix X with items, $\theta_i$, and $y_i$ as the columns and persons as the rows. 
 
The predictions were made by matching the sum score of each out-sample observation to a sum score of the in-sample data for each iteration of the methods. These predictions were then compared to the true classification for each observation and the error recorded. The model testing/selection method that produces the lowest overall misclassification rate was considered the best model for classifying under the current circumstances. Each of the model selection procedures split the data into insample and out-sample groups as described previously.
 
### Form Selection 
The sample item difficulties were estimated for each condition with the `eRm` package’s RM function in R (Mair, Hatzinger, & Maier, 2015) with each of the model selection methods. For each of the methods, the parameters estimated on the insample training data (e.g. the N – 1 points in LOOCV) were used to estimate the insample testing person latent abilities. These estimates produced a conversion table of summated scores to latent ability scores, which were then used to estimate the simulated external criterion variable $y$. 

For the first set of models, 10 short form items were selected so that 3 models could be compared: the first with items 1 through 10 (easy items), the second with items 11 through 20 (moderate items), and the last with items 21 through 30 (hard items). The expectation was that, since $y = 1$ for positive thetas and 0 otherwise, the model selection procedures should all select the moderate items subtest (i.e., the subtest that most accurately classified persons according to the external criterion). In addition, each procedure was performed on the full 30-item test and for each test the resubstitution error was calculated. Counting variations on certain procedures, there were a total of 13 model selection procedures used on the simulated data: the leave-one-out crossvalidation procedure, the k-fold cross-validation procedure with k = 5, 10, 15, and 20, the repeated leave-one-out bootstrap with B=5, 10, 15, and 20, the $e_0$ bootstrap, the .632 bootstrap, and the repeated bootstrap k-fold cv with B=5 and k=10. In addition, the resubstitution error, used in the .632 bootstrap, was presented as its own method and true out-sample error for each condition was estimated by using the parameter estimates from the entire in-sample dataset to classify 500 newly-generated persons and response patterns. These procedures were chosen for two reasons. First, the choices are consistent with those seen in the literature on cross-validation methodology (see Chapter 2). Second, the choices are all applicable to short form selection procedures that involve multiple test forms and an external criterion. All fourteen conditions were reiterated for a total of 100 repetitions, with any repetition that failed to converge to a solution for the full in-sample data being restarted. 

## 2PL Model 
### Data Generation 
The second part of the study simulated responses to a thirty item dichotomously scored test using a 2PL model in R. Procedures similar to the Rasch model data generation were used, with the following differences: a) the 30 items, while maintaining the difficulties, had a randomly-generated discrimination parameter generated from $a_j \sim Unif(0.5,1.5)$ used across each iteration (see Table 3-1 for these parameters) and b) different sample sizes were used (i.e., n = 100, 200, and 500) due to the increased sample size requirements of the 2PL model compared to the Rasch model. While a sample size of 500 has been suggested as an ideal minimum for 2PL models by Reeve and Fayers (2005), smaller sample sizes can still provide useful information in some circumstances (e.g., pilot studies) and were thus included in the simulation (Cappelleri et al., 2014). Then a response vector on each of the 30 items for all n was generated by the following steps:

1.	Calculate the probability of a correct response on each item using the 2PL logit formula for each observation j on each item i, using the true item parameters generated previously. 
2.	Generate the response vector for each person by performing a Bernoulli trial with probability $P(Y_{ij} = Correct|\theta_j)$ for each item i. 
 
The R package `ltm` was used throughout the remainder of the simulation to estimate the 2PL item parameters by using the in-sample response vectors from the model selection procedures (Rizopoulos, 2006). These parameters were then used to estimate the out-sample response vector thetas by using the single observation maximum likelihood estimation procedure found in chapter 5 of Baker (2001) such that 
(equation 3-2 here) 
where $\hat{\theta}_s$ is the *s*th’s iteration theta estimate, $a_i$ is the discrimination of item *i*, N is the 
total number of items, $\mu_i$ is the observed response to item *i*, and $P_i(\hat{\theta}_s = 1 - Q_i(\hat{\theta}_s$ is the probability of correct response under the given item parameters at ability $\hat{\theta}_s$. The stopping parameters for this procedure were 1) $|\hat{\theta}_{s+1} - \hat{\theta}_s| < .001$ 2) $|\hat{theta}_s| < 10$ and 3) 
$s < 100$ with the initial value of $\hat{\theta}_s$ set to 1. 

### Form Selection 

The subtests were created in the same manner as was done for the Rasch tests, with each set of ten items being used to create the short forms. Based on the results of the Rasch test conditions, the following four of the previous thirteen procedures were excluded from this simulation: the 15-Fold CV, the 15 Repeated Bootstrap, the $e_0$ Bootstrap, and the .632 Bootstrap. Again, the true out-sample error is estimated by taking the parameters estimated with the full in-sample dataset and classifying 500 newly-generated persons and response patterns. 

## Evaluation of Results 

The same evaluation methodology was used across the two test models. The mean error rates were calculated for each condition across the 100 iterations. Each of the procedures has its own error estimation, each of which was specified in Chapter 2 (see Equations 2-4, 2-5, 2-7, 2-8, 2-9, and 2-10). Then, the bias was estimated by subtracting the mean out-sample error, estimated from the 500 from the other model mean estimates and summing the resulting values. This is performed within each sample size in order to see how sample size affects the bias, and then again across all sample sizes to find an overall bias. Two sets of graphs were also produced. The first set showed the distribution of the change in error estimation across each condition for each method and the second showed how the distribution of the error changed across each method for each condition. 
